{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](../notebooks/images/header4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop goals\n",
    "* MNIST dense model review\n",
    "* Generating new data\n",
    "* Convolutional networks\n",
    "* Using pretrained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to get and open this file:\n",
    "1. `git pull`\n",
    "1. `jupyter notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's revisit the MNIST example\n",
    "\n",
    "Here's the dense network we used with two layers, the first using relu activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "orig_image = train_images[0]\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
    "\n",
    "reshaped_image = train_images[0]\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(784,)))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "network.fit(train_images, train_labels, epochs=10, batch_size=256);\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allowing for some randomness, running this model for 10 epochs gives a training accuracy of 99.6% and a testing accuracy of 97.9% percent. Further training (up to about 30 epochs) gives a training accuracy of 100% and a testing accuracy of 98%. Even after 10 epochs it's overfit. This is also a fairly easy problem - clean data, small images, greyscale, only 10 classes, etc. We'll need a new technique to get better results.\n",
    "\n",
    "# Losing information\n",
    "\n",
    "The images were originally 28 by 28 pixel images. As part of our data preparation for a dense network, we flatten or reshaped them into linear arrays of 784 numbers. However, this is throwing away important information - pixel clumps clearly have information that we are losing by reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of original images: {orig_image.shape}\")\n",
    "print(f\"Reshaped to linear array: {reshaped_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(orig_image, cmap='Greys');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the top bar of the 5 in both formats to compare: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(orig_image[5:8,:], cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reshaped_image[140:225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dense model that we trained above is able to use individual pixels and patterns of pixels but is not able to detect translations and rotations. If the network sees a slightly different `5` than is expects, it probably won't correctly classify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_image = np.zeros([28,28])\n",
    "trans_image[:24, 4:] = orig_image[4:, :24]\n",
    "\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "ax1 = f.add_subplot(121)\n",
    "ax1.imshow(orig_image, cmap='Greys')\n",
    "ax1.set_title(\"Original\")\n",
    "\n",
    "ax2 = f.add_subplot(122)\n",
    "ax2.imshow(trans_image, cmap='Greys')\n",
    "ax2.set_title(\"Smoothed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible approach is to programmatically generate different versions of the current images. The Keras image library has lots of utilities for working with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "data_generator = image.ImageDataGenerator(rotation_range=40, width_shift_range=0.2, height_shift_range=0.2)\n",
    "orig_image_rank_4 = orig_image.reshape((1,28,28,1))\n",
    "batch = data_generator.flow(orig_image_rank_4, batch_size=1)\n",
    "plt.imshow(batch[0].reshape(28,28), cmap='Greys')\n",
    "plt.title(\"Random translation and rotation\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can help in some situations but since we aren't really generating new training data, just preturbations of the current training data, we will probably still have overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks\n",
    "\n",
    "### The convolution operation in image processing\n",
    "\n",
    "The main idea behind convolution is that we explicitly encode pixels patterns by moving **kernels** aka **filter windows** over the image, creating output features. What do I mean by kernels?\n",
    "\n",
    "A kernel is just a matrix of the same shape as our input data - in this case a 2d matrix. It's usually much smaller in size than the data, 3 x 3 is a common size.\n",
    "\n",
    "The output image is calculated by sliding the kernel over the input data and calculating the sum of the surrounding pixels, weighted by the corresponding values of the kernal. This is not a standard matrix multiplication.\n",
    "\n",
    "Example calculation of the entry in first row,first column output:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 3 & 4 & 2 & 4 \\\\\n",
    "0 & 1 & 4 & 1 & 1 \\\\\n",
    "1 & 1 & 0 & 7 & 2 \\\\\n",
    "3 & 2 & 3 & 1 & 1 \\\\\n",
    "1 & 2 & 0 & 2 & 8\n",
    "\\end{pmatrix}\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "-1 & 8 & -1 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix}\n",
    "=>\n",
    "\\begin{pmatrix}\n",
    " -7 & . & . \\\\\n",
    " . & . & . \\\\\n",
    " . & . & .\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "where the actual calculation is $-2 + -3 + -4 + 8 + -4 + -1 + -1 = -7$. The dots in the output matrix are filled in by sliding kernel to a different part of the input matrix. \n",
    "\n",
    "Note that the output matrix is smaller than the input matrix. This is because the kernal can only fit in the input matrix in only 9 unique places. This is called a **valid padding**. \n",
    "\n",
    "This is not the only choice - we can also have a **same padding**, where we add the input matrix with enough zeros on all sides so that output matrix is the same size as the input matrix:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ \n",
    "0 & 2 & 3 & 4 & 2 & 4 & 0 \\\\\n",
    "0 & 0 & 1 & 4 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 & 0 & 7 & 2 & 0 \\\\\n",
    "0 & 3 & 2 & 3 & 1 & 1 & 0 \\\\\n",
    "0 & 1 & 2 & 0 & 2 & 8 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0  \n",
    "\\end{pmatrix}\n",
    "*\n",
    "\\begin{pmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "-1 & 8 & -1 \\\\\n",
    "-1 & -1 & -1\n",
    "\\end{pmatrix}\n",
    "=>\n",
    "\\begin{pmatrix}\n",
    " . & . & . & . & . \\\\\n",
    " . & -7 & . & . & . \\\\\n",
    " . & . & . & . & . \\\\\n",
    " . & . & . & . & . \\\\\n",
    " . & . & . & . & .\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "There's yet another choice we can make that affect the size of the output matrix: **strides**. We may choose to move the kernel over more than one entry, i.e. skip entries. \n",
    "\n",
    "For Keras Conv2D layers the default values are `strides=1, padding='valid'`\n",
    "\n",
    "Image processing gives us several important filters to reduce image noise, find edges, etc ... In the interest of time, we'll skip are more in-depth explanation of these work but there are many good explanations a Google search away. Instead, I'll just demonstrate some of the more popular kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the convolutions\n",
    "\n",
    "smoothing_kernel = np.array([[1,1,1],[1,1,1],[1,1,1]])\n",
    "smooth = convolve2d(orig_image, smoothing_kernel)\n",
    "\n",
    "horizontal_edge = np.array([[-1,-1,-1],[2,2,2],[-1,-1,-1]])\n",
    "h_edge = convolve2d(orig_image, horizontal_edge, 'same')\n",
    "\n",
    "vertical_edge = np.array([[-1,2,-1],[-1,2,-1],[-1,2,-1]])\n",
    "v_edge = convolve2d(orig_image, vertical_edge, 'same')\n",
    "\n",
    "edge_detector = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\n",
    "edge = convolve2d(orig_image, edge_detector, 'same')\n",
    "\n",
    "# Plot the resulting images\n",
    "\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "ax1 = f.add_subplot(221)\n",
    "ax1.imshow(orig_image, cmap='Greys')\n",
    "ax1.set_title(\"Original\")\n",
    "\n",
    "ax2 = f.add_subplot(222)\n",
    "ax2.imshow(smooth, cmap='Greys')\n",
    "ax2.set_title(\"Smoothed\")\n",
    "\n",
    "ax3 = f.add_subplot(223)\n",
    "ax3.imshow(h_edge, cmap='Greys')\n",
    "ax3.set_title(\"Horizontal Edges\")\n",
    "\n",
    "ax4 = f.add_subplot(224)\n",
    "ax4.imshow(edge, cmap='Greys')\n",
    "ax4.set_title(\"All Edges\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice property of these image filter kernels is that they are able to detect features no matter where they are in the image. Look at the edge detectors above - all the horizontal edges are highlighted. Hopefully, this will help us build translationally invariant image classifiers!\n",
    "\n",
    "There is a drawback of course. These kernels are predefined. We want to learn good kernals from our data. We can do this with convolution layers in Keras.\n",
    "\n",
    "Let's build a model and then we'll explain the steps in more detail.\n",
    "\n",
    "# MNIST with convolution layers\n",
    "\n",
    "A demonstration first, then we'll switch to a different data set while we explain what is happening in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "network.add(layers.MaxPooling2D((2,2)))\n",
    "network.add(layers.Conv2D(64, (3,3), activation='relu')) \n",
    "network.add(layers.MaxPooling2D((2,2)))\n",
    "network.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "network.add(layers.Flatten())\n",
    "network.add(layers.Dense(64, activation='relu', input_shape=(784,)))\n",
    "# network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = network.fit(train_images,\n",
    "                      train_labels,\n",
    "                      epochs=2,\n",
    "                      batch_size=64,\n",
    "                      validation_data=(test_images, test_labels));\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside - GPU training\n",
    "\n",
    "You'll notice this is much slower - many more calculations are happening! If we make the networks even larger, or train on larger images or have more of them, we'll quickly get to a point where training time is the greatest barrier to training better models. Ultimately, you'll want to use a GPU to train models.\n",
    "\n",
    "You'll have to install some GPU specific mathematical calculation libraries and a GPU specific version of tensorflow. Look for online resources for this.\n",
    "\n",
    "Sources for GPU training:\n",
    "1. Use a local NVIDIA GPU\n",
    "1. Online using Google Colab - hosted Jupyter notebooks with free GPUs\n",
    "1. Online using AWS\n",
    "1. Other cloud providers - IBM, paperspace - A quick glance at prices show online providers are 25 cents/hour and up.\n",
    "1. Don't train models from scratch!\n",
    "\n",
    "### Let's look at the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "val_acc = history.history['val_acc']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original MLP validation error = 1.6%, CNN validation error = 0.8%. So we've cut the error in half with our first attempt at building at CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the convolution layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [network.layers[i].output for i in [0,2,4]]\n",
    "activation_model = models.Model(inputs=network.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(test_images[image_number].reshape((1,28,28,1)))\n",
    "layer_names = [ network.layers[i].name for i in [0,2,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the kernel initialization is random, your feature maps won't look exactly like mine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 151\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_images[image_number].reshape((28,28)), cmap='Greys');\n",
    "plt.title(\"Original\")\n",
    "plt.subplot(1,2,2)\n",
    "activations = activation_model.predict(test_images[image_number].reshape((1,28,28,1)))\n",
    "plt.imshow(activations[0][0, :, :, 5])\n",
    "plt.title(\"Activation\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_row = 16\n",
    "\n",
    "# Now let's display our feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # We will tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # We'll tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first conv layer tends to be visually pretty close to the original image and looks similar to the edge detector filters.\n",
    "\n",
    "The second layer is more abstract but some still show hints of image. By looking at a bunch of different images we might be able to identify filters that detect certain shapes, e.g. circles, horizontal or vertical lines.\n",
    "\n",
    "By the third layer it's almost impossible to tell what the original number was. The data has been turned into a set of abstract features. We also see more filters as blank, indicating that they weren't activated at all by this input image. \n",
    "\n",
    "Since the last convolution layer outputs a bunch of abstract features, we need to add a small dense network on top to finish the actual classification.\n",
    "\n",
    "The **Flatten** layer simply unrolls the 2D image data into the linear array that the Dense layers expect, much like the **reshape** function we've used a few times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for further exploration\n",
    "\n",
    "### Use pretrained models!\n",
    "\n",
    "We found that training even our modest models on a few thousand images takes a few minutes. Better models have more layers, bigger layers and can have order of magnitude more parameters and take hours or days to train. Why do that when we can use pretrained models. These models are tuned to turn image data into features. We can then build our own dense layers on top of the convolution layers.\n",
    "\n",
    "Some of these models are already defined in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define new dense layers on top of this, **freeze** the VGG16 layer weights, and then train the new model on our new data. \n",
    "\n",
    "This is called **transfer learning**.\n",
    "\n",
    "Here's an example of how to freeze a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv_base.layers[2].trainable)\n",
    "conv_base.layers[2].trainable = False\n",
    "print(conv_base.layers[2].trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language dataset\n",
    "\n",
    "While MNIST handwriting is a good trial dataset, I'm interested in working on something new. [Kaggle](https://www.kaggle.com) is a machine learning competition site. These competitions often have pretty cool datasets publicly available.\n",
    "\n",
    "I'm interesting in two things: 1) a more challenging dataset and 2) data for my rock-paper-scissors image detector. \n",
    "\n",
    "This [dataset](https://www.kaggle.com/datamunge/sign-language-mnist) has both. It's a bunch of images of american sign language examples. Signing two of the letters incorporates motion, so the data set only uses 24 letters. And, 3 of the letter look like the RPS sysmbols!\n",
    "\n",
    "You will need to create an account on Kaggle to get the data. Once you have the data, unzip it and put the files in the **data/sign-language-mnist/** directory. \n",
    "\n",
    "I've written a helper functions to translate this data into the image data shape expected by Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of American Sign Language letters\n",
    "![test](data/sign-language-mnist/amer_sign2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the data, split into labels and images, reshape data, filter for only 3 classes\n",
    "\n",
    "def rps_load_data(filename):\n",
    "    df = pd.read_csv(filename, delimiter=',')\n",
    "    df = df[df['label'].isin([0,1,21])]\n",
    "    df.replace({\"label\":21}, 2, inplace=True) \n",
    "    labels = df.pop('label')  #Pops the label column and stores in 'labels'\n",
    "    labels = to_categorical(labels)\n",
    "    data = df.values\n",
    "    data = np.array([np.reshape(i, (28,28)) for i in data])\n",
    "    data = data.reshape(data.shape[0], 28,28,1)\n",
    "    data = data / 255\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = rps_load_data(\"data/sign-language-mnist/sign_mnist_train.csv\")\n",
    "test_data, test_labels = rps_load_data(\"data/sign-language-mnist/sign_mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 13\n",
    "plt.imshow(test_data[image_number].reshape((28,28)), cmap='Greys');\n",
    "plt.title(\"Example\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first try at an RPS CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build network here:\n",
    "# How many output classes are there? \n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile and train here:\n",
    "\n",
    "network.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = network.fit(train_data, train_labels, epochs=12, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_data, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first convolutional layer and it's summary looks like this:\n",
    "\n",
    "`Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1))`\n",
    "\n",
    "```\n",
    "_______________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_10 (Conv2D)           (None, 26, 26, 16)        160         \n",
    "```\n",
    "\n",
    "Since it's the first layer we have to tell it that we are using 28 x 28 images with only 1 color. RGB color images would be (28, 28, 3).\n",
    "\n",
    "We are specifying that we want to create 32 kernels, each 3 x 3. Each kernel leads to an output neuron with the relu activation. Thus we are learning 10 values for each kernel - 9 for the kernel itself and one weight, 320 parameters in total.\n",
    "\n",
    "What about the output shape? The first entry is the batch size, which is variable, and so is listed as None. The other numbers represent the 26 x 26 images (shruken by the convolution) and the outputs of the 32 convolutions.\n",
    "\n",
    "Notice we really expanded the amount of data. Each input image has turned into 32 slightly smaller outputs images after the first layer. This has the effect of increasing training time significantly. Max pooling is a way to decrease the number of parameters. In this case we simply tell the layer to split each image up into 2 by 2 blocks and output the max value.\n",
    "\n",
    "MaxPooling also helps the network detect slightly translated features and to generalize.\n",
    "\n",
    "Next we have two more convolution layers, separated by a max pooling layer. \n",
    "\n",
    "Why do we have multiple convolution layers? It's because each successive layer can learn more and more complex shapes. The first layer is looking for 3 x 3 patterns in the images - curves or straight lines. The second layer looks for combinations of the first layer features - like a circle. The final layer will look for even more complocated features. Convolutional layers can learn spacial heirarchies.\n",
    "\n",
    "Let's examine that by putting a single image into our trained network and examining the filter outputs of each convolution layer. - code adapted from *Deep Learning with Python* by Francois Chollet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to try out:\n",
    "1. Try different training parameters to improve the RPS model.\n",
    "1. Change the rps_load_data function to output data for all the classes. Create and train a network that predicts all 24 classes. \n",
    "1. Generate new data using opencv and your webcam."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
